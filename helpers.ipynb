{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import re\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from os import listdir, environ\n",
    "import pandas as pd\n",
    "from IPython import embed\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datasets import dataset_utils\n",
    "from helpers import *\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def update_screen(msg):\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def cats_vs_dogs_training_data():\n",
    "    TRAIN_DIR = '/home/devyhia/cats.vs.dogs/train/'\n",
    "    TEST_DIR = '/home/devyhia/cats.vs.dogs/test/'\n",
    "\n",
    "    ROWS = 299\n",
    "    COLS = 299\n",
    "    CHANNELS = 3\n",
    "    SLICE = 10000\n",
    "\n",
    "    train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "    train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "    train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "    # test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "\n",
    "    # slice datasets for memory efficiency on Kaggle Kernels, delete if using full dataset\n",
    "    train_images = train_dogs[:SLICE] + train_cats[:SLICE]\n",
    "    valid_images = train_dogs[SLICE:] + train_cats[SLICE:]\n",
    "\n",
    "    np.random.shuffle(train_images)\n",
    "    np.random.shuffle(valid_images)\n",
    "\n",
    "    def read_image(file_path):\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "        return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    def assign_image(data, i, image_file):\n",
    "        data[i] = read_image(image_file).astype(np.float32)\n",
    "\n",
    "    def prep_data(images):\n",
    "        count = len(images)\n",
    "        data = np.ndarray((count, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "        \n",
    "        ts = []\n",
    "        for i, image_file in enumerate(images):\n",
    "#             data[i] = read_image(image_file).astype(np.float32)\n",
    "            t = threading.Thread(target=assign_image, args = (data,i,image_file))\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "            ts.append(t)\n",
    "            \n",
    "        for i,t in enumerate(ts):\n",
    "            t.join()\n",
    "            update_screen('\\rProcessed {} of {}'.format(i, len(ts)))\n",
    "        \n",
    "        update_screen('\\n')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_label(path):\n",
    "        return 1 if re.search(\"(cat|dog)\\.(\\d+)\\.\", path).group(1) == 'cat' else 0\n",
    "    \n",
    "    print(\"Prep data ...\")\n",
    "    X = prep_data(train_images)\n",
    "    Xt = prep_data(valid_images)\n",
    "    # test = prep_data(test_images)\n",
    "    \n",
    "#     pre_processing = tf.placeholder(tf.float32, shape=[None, 299,299,3])\n",
    "#     after_processing = tf.sub(tf.mul(pre_processing, 2.0/255.), 1.0)\n",
    "    \n",
    "#     def preprocess(sess, X, i, batch_size):\n",
    "#         update_screen(\"\\rPreprocessing {} ...\".format(i))\n",
    "#         return sess.run(after_processing, feed_dict={pre_processing: X[i*batch_size:(i+1)*batch_size]})\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         Xs, Xts = [], []\n",
    "#         batch_size = 200\n",
    "        \n",
    "#         for i in range(X.shape[0] / batch_size):\n",
    "#             Xs.append(preprocess(sess, X, i, batch_size))\n",
    "#         update_screen(\"\\n\")\n",
    "        \n",
    "#         for i in range(Xt.shape[0] / batch_size):\n",
    "#             Xts.append(preprocess(sess, Xt, i, batch_size))\n",
    "#         update_screen(\"\\n\")\n",
    "        \n",
    "#         X = np.vstack(Xs)\n",
    "#         Xt = np.vstack(Xts)\n",
    "        \n",
    "    print(\"Train shape: {}\".format(X.shape))\n",
    "    print(\"Valid shape: {}\".format(Xt.shape))\n",
    "\n",
    "    labels_train = [get_label(i) for i in train_images]\n",
    "    labels_valid = [get_label(i) for i in valid_images]\n",
    "\n",
    "    print(pd.DataFrame(labels_train, columns=[\"label\"])[\"label\"].value_counts())\n",
    "    print(pd.DataFrame(labels_valid, columns=[\"label\"])[\"label\"].value_counts())\n",
    "\n",
    "    y = np.zeros((X.shape[0], 2))\n",
    "    yt = np.zeros((Xt.shape[0], 2))\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i, labels_train[i]] = 1\n",
    "\n",
    "    for i in range(yt.shape[0]):\n",
    "        yt[i, labels_valid[i]] = 1\n",
    "\n",
    "    # print(labels_train)\n",
    "    # print(labels_valid)\n",
    "\n",
    "    print(y)\n",
    "    print(yt)\n",
    "\n",
    "    print(\"X=\", X.shape, \"y=\", y.shape)\n",
    "    print(\"Xt=\", Xt.shape, \"yt=\", yt.shape)\n",
    "\n",
    "    return X, y, Xt, yt\n",
    "\n",
    "def cats_vs_dogs_testing_data(dim=224):\n",
    "    CACHE_PATH = \"cache/Xt.npy\"\n",
    "    if os.path.isfile(CACHE_PATH):\n",
    "        data = np.load(CACHE_PATH)\n",
    "        return range(1, data.shape[0]+1), data\n",
    "\n",
    "    TEST_DIR = '/home/devyhia/cats.vs.dogs/test/'\n",
    "\n",
    "    ROWS = dim\n",
    "    COLS = dim\n",
    "    CHANNELS = 3\n",
    "\n",
    "    file_key = lambda f: int(re.match(\"(\\d+)\\.jpg\", f).group(1))\n",
    "    test_images =  [TEST_DIR+i for i in sorted(os.listdir(TEST_DIR), key=file_key)]\n",
    "\n",
    "    def read_image(file_path):\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "        return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "    def prep_data(images):\n",
    "        count = len(images)\n",
    "        data = np.ndarray((count, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "        ids = []\n",
    "\n",
    "        for i, image_file in enumerate(images):\n",
    "            data[i] = read_image(image_file)\n",
    "            ids.append(re.search(\"(\\d+)\\.\", image_file).group(1))\n",
    "\n",
    "            if i%250 == 0: update_screen('\\rProcessed {} of {}'.format(i, count))\n",
    "        \n",
    "        update_screen('\\n')\n",
    "        \n",
    "        return ids, data\n",
    "\n",
    "    ids, data = prep_data(test_images)\n",
    "\n",
    "    np.save(CACHE_PATH, data)\n",
    "\n",
    "    return ids, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SlimModel:\n",
    "    def __init__(self, model_name, checkpoint_name, checkpoint_dir, dim, num_classes, isTraining=True, checkpoint_exclude_scopes=[\"InceptionV4/Logits\", \"InceptionV4/AuxLogits\"]):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.checkpoint_name = checkpoint_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.dim = dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, self.dim,self.dim,3])\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        self.isTraining = isTraining\n",
    "\n",
    "        self.ImageNetMean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "\n",
    "        self.X_Norm = self.X - self.ImageNetMean\n",
    "\n",
    "        self.name = model_name\n",
    "        self.best_accuracy = tf.Variable(0.0, trainable=False, name='best_accuracy')\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.summaries = []\n",
    "        \n",
    "        self.checkpoint_exclude_scopes= checkpoint_exclude_scopes\n",
    "\n",
    "        # Just in case a previous run was closed using a halt file.\n",
    "        if os.path.isfile(\"{}.halt\".format(self.name)):\n",
    "            os.remove(\"{}.halt\".format(self.name))\n",
    "        \n",
    "    def __get_init_fn(self):\n",
    "        \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "        exclusions = [scope.strip() for scope in self.checkpoint_exclude_scopes]\n",
    "\n",
    "        variables_to_restore = []\n",
    "        for var in slim.get_model_variables():\n",
    "            excluded = False\n",
    "            for exclusion in exclusions:\n",
    "                if var.op.name.startswith(exclusion):\n",
    "                    excluded = True\n",
    "                    break\n",
    "            if not excluded:\n",
    "                variables_to_restore.append(var)\n",
    "\n",
    "        return slim.assign_from_checkpoint_fn(os.path.join(self.checkpoint_dir, self.checkpoint_name), variables_to_restore)\n",
    "    \n",
    "    def model(self, logits, end_points, pred_key=\"Predictions\"):\n",
    "        self.logits = logits\n",
    "        self.end_points = end_points\n",
    "        \n",
    "        self.total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n",
    "\n",
    "        self.probs = self.end_points[pred_key]\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.probs, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.summaries += [tf.histogram_summary('Correct Predictions', tf.cast(self.correct_prediction, tf.int32))]\n",
    "        self.summaries += [tf.histogram_summary('Predictions', tf.argmax(self.probs, 1))]\n",
    "        self.summaries += [tf.histogram_summary('Training Labels', tf.argmax(self.y, 1))]\n",
    "        self.summaries += [tf.histogram_summary('Activations', self.probs)]\n",
    "\n",
    "        self.summaries += [tf.scalar_summary('Loss', self.total_loss)]\n",
    "        self.summaries += [tf.scalar_summary('Accuracy', self.accuracy)]\n",
    "        self.summaries += [tf.scalar_summary('Learning Rate', self.learning_rate)]\n",
    "        self.summaries += [tf.scalar_summary('Best Accuracy', self.best_accuracy)]\n",
    "\n",
    "        # Specify the optimizer and create the train op:\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_step = slim.learning.create_train_op(self.total_loss, self.optimizer)\n",
    "\n",
    "        self.init_fn = self.__get_init_fn()\n",
    "\n",
    "    def __iterate_minibatches(self, X,y, size):\n",
    "        if X.shape[0] % size > 0:\n",
    "            raise \"The minibatch size should be a divisor of the batch size.\"\n",
    "\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(idx) # in-place shuffling\n",
    "        for i in range(X.shape[0] / size):\n",
    "            # To randomize the minibatches every time\n",
    "            _idx = idx[i*size:(i+1)*size]\n",
    "            yield X[_idx], y[_idx]\n",
    "\n",
    "    def __update_screen(self, msg):\n",
    "        sys.stdout.write(msg)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def resize_images(self, X):\n",
    "        return np.array([ imresize(X[i], (self.dim,self.dim)) for i in range(X.shape[0])])\n",
    "\n",
    "    def calculate_loss(self, sess, Xt, yt, size=1000, step=10):\n",
    "        fc3ls = None\n",
    "        sample_idx = random.sample(range(0, Xt.shape[0]), size)\n",
    "        for i in range(size / step):\n",
    "            [fc3l] = sess.run([self.logits], feed_dict={self.X: Xt[sample_idx[i*step:(i+1)*step]], self.y: yt[sample_idx[i*step:(i+1)*step]]})\n",
    "            if i == 0:\n",
    "                fc3ls = fc3l\n",
    "            else:\n",
    "                fc3ls = np.vstack((fc3ls, fc3l))\n",
    "\n",
    "        loss, accuracy, summary = sess.run([self.total_loss, self.accuracy, self.tf_summary], feed_dict={self.logits: fc3ls, self.y: yt[sample_idx]})\n",
    "\n",
    "        return loss, accuracy, summary\n",
    "\n",
    "    def __graceful_halt(self, t_start_training):\n",
    "        # Save Trained Model\n",
    "        p = self.saver.save(sess, \"{}.tfmodel\".format(self.name))\n",
    "        t = time.time() - t_start_training\n",
    "        print(\"++ Training: END -- Exec. Time={0:.0f}m {1:.0f}s Model Path={2:s}\".format(t / 60, t % 60, p))\n",
    "\n",
    "    def train(self, sess, X, y, val_X, val_y, learning_rate=0.01, epochs=30, minibatch_size=500, optimizer=None):\n",
    "        print(\"++ Training with {} epochs and {} minibatch size.\".format(epochs, minibatch_size))\n",
    "\n",
    "        BEST_ACC = 0\n",
    "\n",
    "        self.tf_summary = tf.merge_summary(self.summaries)\n",
    "        self.tf_logs = tf.train.SummaryWriter(\"logs/{}\".format(self.name), sess.graph, flush_secs=30)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        # Initialize Model ...\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        self.init_fn(sess)\n",
    "\n",
    "        # Resize Validation Once ...\n",
    "        val_X_res = self.resize_images(val_X)\n",
    "\n",
    "        # I cap the learning rate at 0.001\n",
    "        lr_step = (learning_rate - 0.0001) / epochs\n",
    "        lr = learning_rate + lr_step\n",
    "        step = 0\n",
    "\n",
    "        t_start_training = time.time()\n",
    "        for i in range(epochs):\n",
    "            t_start_epoch = time.time()\n",
    "            t_start_minibatch = time.time()\n",
    "\n",
    "            lr -= lr_step\n",
    "\n",
    "            print(\"+++ Epoch {:.0f}: START (lr={:.4f})\".format(i, lr))\n",
    "            cnt = 0\n",
    "            for _X, _y in self.__iterate_minibatches(X, y, minibatch_size):\n",
    "                # Resize Training Batches (to avoid consuming much memeory)\n",
    "                cnt += 1\n",
    "                _X_res = self.resize_images(_X)\n",
    "                self.__update_screen(\"\\r++++ Mini Batch ({} out of {}): \".format(cnt, X.shape[0]/minibatch_size))\n",
    "                sess.run([self.train_step], feed_dict={self.X: _X_res, self.y: _y, self.learning_rate: lr})\n",
    "\n",
    "                if cnt % 25 == 0:\n",
    "                    # loss, accuracy = sess.run([self.cross_entropy, self.accuracy], feed_dict={self.X: val_X_res[test_sample], self.y: val_y[test_sample]})\n",
    "                    val_loss, val_accuracy, val_summary = self.calculate_loss(sess, val_X_res, val_y, 1000, learning_rate=lr)\n",
    "                    t = time.time() - t_start_minibatch\n",
    "                    self.__update_screen(\" Loss={0:.4f} Accuracy={1:.4f} Exec. Time={2:.0f}m {3:.0f}s\\n\".format(val_loss, val_accuracy, t / 60, t % 60))\n",
    "                    t_start_minibatch = time.time()\n",
    "\n",
    "                    # Save Summary\n",
    "                    step += 1\n",
    "                    self.tf_logs.add_summary(val_summary, step)\n",
    "\n",
    "                    # Handle Close Signals\n",
    "                    if os.path.isfile(\"{}.halt\".format(self.name)):\n",
    "                        self.__graceful_halt(t_start_training)\n",
    "                        os.remove(\"{}.halt\".format(self.name))\n",
    "                        exit(0)\n",
    "\n",
    "            self.__update_screen(\"\\n\")\n",
    "            val_loss, val_accuracy, val_summary = self.calculate_loss(sess, val_X_res, val_y, val_X.shape[0], learning_rate=lr)\n",
    "            t = time.time() - t_start_epoch\n",
    "            print(\"+++ Epoch {:.0f}: END -- Loss={:.4f} Accuracy={:.4f} Exec. Time={:.0f}m {:.0f}s\".format(i, val_loss, val_accuracy, t / 60, t % 60))\n",
    "\n",
    "            # Save Summary\n",
    "            step += 1\n",
    "            self.tf_logs.add_summary(val_summary, step)\n",
    "\n",
    "            # Always Save Best Accuracy\n",
    "            if val_accuracy > BEST_ACC:\n",
    "                BEST_ACC = val_accuracy\n",
    "                self.best_accuracy.assign(BEST_ACC)\n",
    "                self.saver.save(sess, \"{}.tfmodel\".format(self.name))\n",
    "                print(\"+++ Epoch {0:.0f}: END -- SAVED BEST ACC\".format(i))\n",
    "\n",
    "        self.__graceful_halt(t_start_training)\n",
    "\n",
    "\n",
    "    def predict(self, sess, X):\n",
    "        prob = sess.run(self.probs, feed_dict={self.X: [X]})[0]\n",
    "        preds = (np.argsort(prob)[::-1])[0:5]\n",
    "        for p in preds:\n",
    "            print p, prob[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
