{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from os import listdir, environ\n",
    "import pandas as pd\n",
    "from IPython import embed\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "from datasets import dataset_utils\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='RNN-CNN Network.')\n",
    "parser.add_argument('--depth', default=1, help='Depth of the RNN network')\n",
    "parser.add_argument('--hidden', default=128, help='Hidden units of the RNN network')\n",
    "parser.add_argument('--gpu', default=3, help='GPU to use for train')\n",
    "parser.add_argument('--name', default=\"inception_rnn_unit\", help='Name of the RNN model to use for train')\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "\n",
    "BEST_ACC = 0\n",
    "CHECKPOINTS_DIR = \"checkpoints/\"\n",
    "\n",
    "class InceptionV4:\n",
    "    def __init__(self, model_name, isTesting=False):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, 299,299,3], name=\"X\")\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, 100], name=\"y\")\n",
    "\n",
    "        self.name = model_name\n",
    "        \n",
    "        self.__model()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Just in case a previous run was closed using a halt file.\n",
    "        if os.path.isfile(\"{}.halt\".format(self.name)):\n",
    "            os.remove(\"{}.halt\".format(self.name))\n",
    "\n",
    "        # if weights is not None and sess is not None:\n",
    "        #     self.load_weights(sess, weights)\n",
    "    \n",
    "    def __get_init_fn(self):\n",
    "        \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "        checkpoint_exclude_scopes=[\"InceptionV4/Logits\", \"InceptionV4/AuxLogits\", \"RNN_Logits\"]\n",
    "\n",
    "        exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "        variables_to_restore = []\n",
    "        for var in slim.get_model_variables():\n",
    "            excluded = False\n",
    "            for exclusion in exclusions:\n",
    "                if var.op.name.startswith(exclusion):\n",
    "                    excluded = True\n",
    "                    break\n",
    "            if not excluded:\n",
    "                variables_to_restore.append(var)\n",
    "\n",
    "        return slim.assign_from_checkpoint_fn(\n",
    "          os.path.join(CHECKPOINTS_DIR, 'inception_v4.ckpt'),\n",
    "          variables_to_restore)\n",
    "        \n",
    "    def __model(self):\n",
    "        self.image_size = inception.inception_v4.default_image_size\n",
    "\n",
    "        # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "        with slim.arg_scope(inception.inception_v4_arg_scope()):\n",
    "            self.logits, self.end_points = inception.inception_v4(self.X, num_classes=100, is_training=True)\n",
    "            \n",
    "        # Parameters\n",
    "        learning_rate = 0.001\n",
    "        batch_size = 50\n",
    "        display_step = 25\n",
    "        epochs = 100\n",
    "        depth = int(args.depth)\n",
    "\n",
    "        # Network Parameters\n",
    "        n_input = 128 # MNIST data input (img shape: 28*28)\n",
    "        n_steps = 12 # timesteps\n",
    "        n_hidden = int(args.hidden) # hidden layer num of features\n",
    "        n_classes = 100 # MNIST total classes (0-9 digits)\n",
    "        \n",
    "        def RNN(x):\n",
    "            # Prepare data shape to match `rnn` function requirements\n",
    "            # Current data input shape: (batch_size, n_steps, n_input)\n",
    "            # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "            # Permuting batch_size and n_steps\n",
    "            x = tf.transpose(x, [1, 0, 2])\n",
    "            # Reshaping to (n_steps*batch_size, n_input)\n",
    "            x = tf.reshape(x, [-1, n_input])\n",
    "            # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "            x = tf.split(0, n_steps, x)\n",
    "\n",
    "            # Define a lstm cell with tensorflow\n",
    "            #     , forget_bias=1.0\n",
    "            lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "            multi_cells = rnn_cell.MultiRNNCell([lstm_cell] * depth, state_is_tuple=True)\n",
    "\n",
    "            # Get lstm cell output\n",
    "            outputs, states = rnn.rnn(multi_cells, x, dtype=tf.float32)\n",
    "            \n",
    "            net = slim.fully_connected(outputs[-1], 100, activation_fn=None, scope='RNN_Logits')\n",
    "\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "            return net\n",
    "\n",
    "        self.rnn_output = RNN(tf.reshape(self.end_points[\"PreLogitsFlatten\"], (-1, n_steps, n_input)))\n",
    "\n",
    "        # Specify the loss function\n",
    "        self.total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.rnn_output, self.y))\n",
    "        self.probs = tf.nn.softmax(self.rnn_output, name='RNNPredictions')\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.probs, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        self.top5_correct_prediction = tf.nn.in_top_k(self.probs, tf.argmax(self.y,1), 5)\n",
    "        self.top5_accuracy = tf.reduce_mean(tf.cast(self.top5_correct_prediction, tf.float32))\n",
    "\n",
    "    def __iterate_minibatches(self, X,y, size):\n",
    "        if X.shape[0] % size > 0:\n",
    "            raise \"The minibatch size should be a divisor of the batch size.\"\n",
    "\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(idx) # in-place shuffling\n",
    "        for i in range(X.shape[0] / size):\n",
    "            # To randomize the minibatches every time\n",
    "            _idx = idx[i*size:(i+1)*size]\n",
    "            yield X[_idx], y[_idx]\n",
    "\n",
    "    def __update_screen(self, msg):\n",
    "        sys.stdout.write(msg)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def resize_images(self, X):\n",
    "        return np.array([ imresize(X[i], (299,299)) for i in range(X.shape[0])])\n",
    "\n",
    "    def calculate_loss(self, sess, Xt, yt, size=1000, step=10):\n",
    "        fc3ls = []\n",
    "        sample_idx = random.sample(range(0, Xt.shape[0]), size)\n",
    "        for i in range(size / step):\n",
    "            [fc3l] = sess.run([self.rnn_output], feed_dict={self.X: Xt[sample_idx[i*step:(i+1)*step]], self.y: yt[sample_idx[i*step:(i+1)*step]]})\n",
    "            fc3ls.append(fc3l)\n",
    "        \n",
    "        fc3ls = np.vstack(fc3ls)\n",
    "        loss, accuracy, top5accuracy = sess.run([self.total_loss, self.accuracy, self.top5_accuracy], feed_dict={self.rnn_output: fc3ls, self.y: yt[sample_idx]})\n",
    "\n",
    "        return loss, accuracy, top5accuracy\n",
    "\n",
    "    def __graceful_halt(self, t_start_training):\n",
    "        # Save Trained Model\n",
    "        p = self.saver.save(sess, \"{}.tfmodel\".format(self.name))\n",
    "        t = time.time() - t_start_training\n",
    "        print(\"++ Training: END -- Exec. Time={0:.0f}m {1:.0f}s Model Path={2:s}\".format(t / 60, t % 60, p))\n",
    "\n",
    "\n",
    "    def train(self, sess, X, y, val_X, val_y, epochs=30, minibatch_size=500, optimizer=None, loadModel=None):\n",
    "        print(\"++ Training with {} epochs and {} minibatch size.\".format(epochs, minibatch_size))\n",
    "        BEST_ACC = 0.0\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "            print(\"+ Setup custom optimizer ...\")\n",
    "        else:\n",
    "            self.optimizer = tf.train.AdamOptimizer(epsilon=0.1)\n",
    "        \n",
    "        print(\"+ Setup train step ...\")\n",
    "        # Specify the optimizer and create the train op:\n",
    "        self.train_step = slim.learning.create_train_op(self.total_loss, self.optimizer)\n",
    "        \n",
    "        print(\"+ Initialize all variables ...\")\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        self.init_fn = self.__get_init_fn()\n",
    "        \n",
    "        # Initialize Model ...\n",
    "        if loadModel:\n",
    "            print(\"+ Load existing model ({}) ...\".format(loadModel))\n",
    "            self.saver.restore(sess, \"{}.tfmodel\".format(loadModel))\n",
    "        else:\n",
    "            print(\"+ Load InceptionV4 pretrained checkpoint ...\")\n",
    "            self.init_fn(sess)\n",
    "        \n",
    "        print(\"+ Resize Validation Images ...\")\n",
    "        # Resize Validation Once ...\n",
    "        val_X_res = self.resize_images(val_X)\n",
    "\n",
    "        t_start_training = time.time()\n",
    "        for i in range(epochs):\n",
    "            t_start_epoch = time.time()\n",
    "            t_start_minibatch = time.time()\n",
    "            print(\"+++ Epoch 1: START\")\n",
    "            cnt = 0\n",
    "            for _X, _y in self.__iterate_minibatches(X, y, minibatch_size):\n",
    "                # Resize Training Batches (to avoid consuming much memeory)\n",
    "                cnt += 1\n",
    "                _X_res = self.resize_images(_X)\n",
    "                self.__update_screen(\"\\r++++ Mini Batch ({} out of {}): \".format(cnt, X.shape[0]/minibatch_size))\n",
    "                sess.run([self.train_step], feed_dict={vgg.X: _X_res, vgg.y: _y})\n",
    "                if cnt % 25 == 0:\n",
    "                    # loss, accuracy = sess.run([self.cross_entropy, self.accuracy], feed_dict={self.X: val_X_res[test_sample], self.y: val_y[test_sample]})\n",
    "                    val_loss, val_accuracy, val_top5acc = self.calculate_loss(sess, val_X_res, val_y, 1000)\n",
    "                    t = time.time() - t_start_minibatch\n",
    "                    self.__update_screen(\" Loss={0:.4f} Accuracy={1:.4f} Top5 Acc={2:.4f} Exec. Time={3:.0f}m {4:.0f}s\\n\".format(val_loss, val_accuracy, val_top5acc, t / 60, t % 60))\n",
    "                    t_start_minibatch = time.time()\n",
    "\n",
    "                    # Handle Close Signals\n",
    "                    if os.path.isfile(\"{}.halt\".format(self.name)):\n",
    "                        self.__graceful_halt(t_start_training)\n",
    "                        os.remove(\"{}.halt\".format(self.name))\n",
    "                        exit(0)\n",
    "\n",
    "            self.__update_screen(\"\\n\")\n",
    "            val_loss, val_accuracy, val_top5acc = self.calculate_loss(sess, val_X_res, val_y, val_X.shape[0])\n",
    "            t = time.time() - t_start_epoch\n",
    "            print(\"+++ Epoch {0:.0f}: END -- Loss={1:.4f} Accuracy={2:.4f} Top5 Acc={3:.4f} Exec. Time={4:.0f}m {5:.0f}s\".format(i, val_loss, val_accuracy, val_top5acc, t / 60, t % 60))\n",
    "\n",
    "            # Always Save Best Accuracy\n",
    "            if val_accuracy > BEST_ACC:\n",
    "                BEST_ACC = val_accuracy\n",
    "                self.saver.save(sess, \"{}.tfmodel\".format(self.name))\n",
    "                print(\"+++ Epoch {0:.0f}: END -- SAVED BEST ACC\".format(i))\n",
    "\n",
    "        self.__graceful_halt(t_start_training)\n",
    "\n",
    "\n",
    "    def predict(self, sess, X):\n",
    "        prob = sess.run(vgg.probs, feed_dict={vgg.X: [X]})[0]\n",
    "        preds = (np.argsort(prob)[::-1])[0:5]\n",
    "        for p in preds:\n",
    "            print p, prob[p]\n",
    "\n",
    "\n",
    "\n",
    "    def load_weights(self, sess, weight_file):\n",
    "        # This includes optimizer variables as well.\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            if i == 30:\n",
    "                break\n",
    "            print i, k, np.shape(weights[k])\n",
    "            sess.run(self.parameters[i].assign(weights[k]))\n",
    "\n",
    "        # Transfer Learning\n",
    "        # Initialize the last fully connected layer\n",
    "        for par in self.parameters[-2:]:\n",
    "            print(\"Param: {}\".format(par.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main ...\n",
      "++ Training with 30 epochs and 50 minibatch size.\n",
      "+ Setup custom optimizer ...\n",
      "+ Setup train step ...\n",
      "+ Initialize all variables ...\n",
      "+ Load InceptionV4 pretrained checkpoint ...\n",
      "+ Resize Validation Images ...\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 0: END -- Loss=4.6194 Accuracy=0.0080 Top5 Acc=0.0480 Exec. Time=1m 14s\n",
      "+++ Epoch 0: END -- SAVED BEST ACC\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 1: END -- Loss=4.6123 Accuracy=0.0140 Top5 Acc=0.0550 Exec. Time=1m 3s\n",
      "+++ Epoch 1: END -- SAVED BEST ACC\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 2: END -- Loss=4.6096 Accuracy=0.0150 Top5 Acc=0.0520 Exec. Time=1m 3s\n",
      "+++ Epoch 2: END -- SAVED BEST ACC\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 3: END -- Loss=4.6031 Accuracy=0.0060 Top5 Acc=0.0580 Exec. Time=1m 4s\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 4: END -- Loss=4.5976 Accuracy=0.0130 Top5 Acc=0.0660 Exec. Time=1m 8s\n",
      "+++ Epoch 1: START\n",
      "++++ Mini Batch (20 out of 20): \n",
      "+++ Epoch 5: END -- Loss=4.5961 Accuracy=0.0180 Top5 Acc=0.0660 Exec. Time=1m 16s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-203e33e56be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Xt.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtinyImageNetDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/yt.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtinyImageNetDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadeltaOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a776a90a9497>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, X, y, val_X, val_y, epochs, minibatch_size, optimizer, loadModel)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBEST_ACC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mBEST_ACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{}.tfmodel\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+++ Epoch {0:.0f}: END -- SAVED BEST ACC\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph)\u001b[0m\n\u001b[1;32m   1285\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1286\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1750\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/training_util.pyc\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1054\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1055\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1068\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mRepeatedFieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlocal_VarintSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeplearners/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mListFields\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     \u001b[0mall_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsPresent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m     \u001b[0mall_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if __name__ == 'main':\n",
    "print(\"Main ...\")\n",
    "DATA_DIR = \"/home/devyhia/vgg/\"\n",
    "\n",
    "model_name = args.name\n",
    "vgg = InceptionV4(model_name)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "tinyImageNetDir = \"/home/devyhia/vgg\"\n",
    "X, y = np.load(\"{}/X.npy\".format(tinyImageNetDir)), np.load(\"{}/y.npy\".format(tinyImageNetDir))\n",
    "Xt, yt = np.load(\"{}/Xt.npy\".format(tinyImageNetDir)), np.load(\"{}/yt.npy\".format(tinyImageNetDir))\n",
    "\n",
    "vgg.train(sess, X, y, Xt, yt, minibatch_size=50, optimizer=tf.train.AdadeltaOptimizer(epsilon=0.1, learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
